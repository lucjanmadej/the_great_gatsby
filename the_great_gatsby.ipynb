{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the most frequent words in F. Scott Fitzgerald's novel, The Great Gatsby, and how often do they occur?\n",
    "\n",
    "In this notebook, we'll scrape the novel from the website Project Gutenberg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import main packages\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A freely available version of the Great Gatsby can be found at Project Gutenberg as an HTML file:\n",
    "https://www.gutenberg.org/files/219/219-h/219-h.htm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Strict//EN\"\n",
      "\"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd\">\n",
      "<html xmlns=\"http://www.w3.org/1999/xhtml\" xml:lang=\"en\" lang=\"en\">\n",
      "<head>\n",
      "<meta http-equiv=\"Content-Type\" content=\"text/html;charset=utf-8\" />\n",
      "<meta http-equiv=\"Content-Style-Type\" content=\"text/css\" />\n",
      "<title>The Project Gutenberg eBook of The Great Gatsby</title>\n",
      "<link rel=\"coverpage\" href=\"images/cover.jpg\" />\n",
      "<style type=\"text/css\">\n",
      "\n",
      "body{\n",
      "margin-left: 20%;\n",
      "margin-right:\n"
     ]
    }
   ],
   "source": [
    "#get the Heart of Darkness HTML, create a request object\n",
    "# link = 'https://www.gutenberg.org/files/219/219-h/219-h.htm'\n",
    "link = 'https://www.gutenberg.org/files/64317/64317-h/64317-h.htm'\n",
    "r = requests.get(link)\n",
    "\n",
    "#set the correct text encoding (UTF-8, according to info in the link)\n",
    "r.encoding = 'utf-8'\n",
    "\n",
    "#extract the HTML from the request object\n",
    "html = r.text\n",
    "\n",
    "#print first 500 characters in html\n",
    "print(html[:500])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Package BeautifulSoup will be used to extract text from html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n a fashion that rather took your breath away: for instance, he’d brought down a string of polo ponies from Lake Forest. It was hard to realize that a man in my own generation was wealthy enough to do that.\n",
      "\n",
      "\n",
      "Why they came East I don’t know. They had spent a year in France for no particular reason, and then drifted here and there unrestfully wherever people played polo and were rich together. This was a permanent move, said Daisy over the telephone, but I didn’t believe it—I had no sight into \n"
     ]
    }
   ],
   "source": [
    "#create a BeautifulSoup object from html\n",
    "soup = BeautifulSoup(html)\n",
    "\n",
    "#get text out of soup\n",
    "text = soup.get_text()\n",
    "\n",
    "#print characters 32500-33000\n",
    "print(text[10000:10500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natural Language Toolkit will be used to tokenize the text and split it into list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Project', 'Gutenberg', 'eBook', 'of', 'The', 'Great', 'Gatsby', 'The', 'Project']\n"
     ]
    }
   ],
   "source": [
    "#creating a tokenizer with regex 'w+' (words)\n",
    "tokenizer = nltk.tokenize.RegexpTokenizer('\\w+')\n",
    "\n",
    "#tokenize the text\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "#print the first 10 tokens\n",
    "print(tokens[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the words lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'project', 'gutenberg', 'ebook', 'of', 'the', 'great', 'gatsby', 'the', 'project']\n"
     ]
    }
   ],
   "source": [
    "#create empty list words\n",
    "words = []\n",
    "\n",
    "#loop through tokens and append their lowercase version to 'words' list\n",
    "for token in tokens:\n",
    "    words.append(token.lower())\n",
    "\n",
    "#print the first 10 words\n",
    "print(words[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "English stop words have to be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['project', 'gutenberg', 'ebook', 'great', 'gatsby', 'project', 'gutenberg', 'ebook', 'great', 'gatsby']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/lucjan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#dowloading and importing stopwords from nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#assign English stopwords to 'sw'\n",
    "sw = stopwords.words('english')\n",
    "\n",
    "#loop through 'words' list and append to 'words_ns' only those\n",
    "#that do not appear in 'sw' list\n",
    "\n",
    "words_ns = []\n",
    "for word in words:\n",
    "    if word not in sw:\n",
    "        words_ns.append(word)\n",
    "        \n",
    "#print the first 10 words\n",
    "print(words_ns[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('gatsby', 268), ('said', 235), ('tom', 191), ('daisy', 186), ('one', 154), ('like', 122), ('man', 114), ('back', 109), ('came', 108), ('little', 103)]\n"
     ]
    }
   ],
   "source": [
    "#Initialize Counter object from list of words_ns\n",
    "counter = Counter(words_ns)\n",
    "\n",
    "#Find top 10 most common words\n",
    "top_10 = counter.most_common(10)\n",
    "\n",
    "#print top 10\n",
    "print(top_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a33f46db87e254908e961f3354589f4be2520d45db94b1f800dda9040a31762f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('book_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
