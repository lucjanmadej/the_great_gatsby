{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **What are the most frequent words in F. Scott Fitzgerald's novel, The Great Gatsby, and how often do they occur?**\n",
    "\n",
    "In this notebook, we'll scrape the novel from the website Project Gutenberg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import main packages\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A freely available version of the Great Gatsby can be found at Project Gutenberg as an HTML file:\n",
    "https://www.gutenberg.org/files/219/219-h/219-h.htm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Strict//EN\"\n",
      "\"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd\">\n",
      "<html xmlns=\"http://www.w3.org/1999/xhtml\" xml:lang=\"en\" lang=\"en\">\n",
      "<head>\n",
      "<meta http-equiv=\"Content-Type\" content=\"text/html;charset=utf-8\" />\n",
      "<meta http-equiv=\"Content-Style-Type\" content=\"text/css\" />\n",
      "<title>The Project Gutenberg eBook of The Great Gatsby</title>\n",
      "<link rel=\"coverpage\" href=\"images/cover.jpg\" />\n",
      "<style type=\"text/css\">\n",
      "\n",
      "body{\n",
      "margin-left: 20%;\n",
      "margin-right:\n"
     ]
    }
   ],
   "source": [
    "# Get The Great Gatsby HTML\n",
    "link = 'https://www.gutenberg.org/files/64317/64317-h/64317-h.htm'\n",
    "# Create a request object\n",
    "r = requests.get(link)\n",
    "\n",
    "# Set the correct text encoding (UTF-8, according to info in the link)\n",
    "r.encoding = 'utf-8'\n",
    "\n",
    "# Extract the HTML from the request object\n",
    "html = r.text\n",
    "\n",
    "# Print first 500 characters in html\n",
    "print(html[:500])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Package BeautifulSoup will be used to extract text from html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n a fashion that rather took your breath away: for instance, he’d brought down a string of polo ponies from Lake Forest. It was hard to realize that a man in my own generation was wealthy enough to do that.\n",
      "\n",
      "\n",
      "Why they came East I don’t know. They had spent a year in France for no particular reason, and then drifted here and there unrestfully wherever people played polo and were rich together. This was a permanent move, said Daisy over the telephone, but I didn’t believe it—I had no sight into \n"
     ]
    }
   ],
   "source": [
    "# Create a BeautifulSoup object from html\n",
    "soup = BeautifulSoup(html)\n",
    "\n",
    "# Get text out of soup\n",
    "text = soup.get_text()\n",
    "\n",
    "# Print some characters\n",
    "print(text[10000:10500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natural Language Toolkit will be used to tokenize the text and split it into list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Project', 'Gutenberg', 'eBook', 'of', 'The', 'Great', 'Gatsby', 'The', 'Project']\n"
     ]
    }
   ],
   "source": [
    "# Create a tokenizer with regex '\\w+' (words)\n",
    "tokenizer = nltk.tokenize.RegexpTokenizer('\\w+')\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "# Print the first 10 tokens\n",
    "print(tokens[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can notice that some words are in lowercase and some are Capitalized. We have to make all of them lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'project', 'gutenberg', 'ebook', 'of', 'the', 'great', 'gatsby', 'the', 'project']\n"
     ]
    }
   ],
   "source": [
    "# Create an empty list words\n",
    "words = []\n",
    "\n",
    "# Loop through tokens and append their lowercase version to 'words' list\n",
    "for token in tokens:\n",
    "    words.append(token.lower())\n",
    "\n",
    "# Print the first 10 words\n",
    "print(words[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some stop words, like 'of' and 'the' can be noticed. English stop words have to be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/lucjan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['project', 'gutenberg', 'ebook', 'great', 'gatsby', 'project', 'gutenberg', 'ebook', 'great', 'gatsby']\n"
     ]
    }
   ],
   "source": [
    "# Download and import stop words from NLTK\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Assign English stop words to 'sw'\n",
    "sw = stopwords.words('english')\n",
    "\n",
    "# Loop through 'words' list\n",
    "# Append to 'words_ns' only those that do not appear in 'sw' list\n",
    "words_ns = []\n",
    "for word in words:\n",
    "    if word not in sw:\n",
    "        words_ns.append(word)\n",
    "        \n",
    "# Print the first 10 words\n",
    "print(words_ns[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('gatsby', 268), ('said', 235), ('tom', 191), ('daisy', 186), ('one', 154), ('like', 122), ('man', 114), ('back', 109), ('came', 108), ('little', 103)]\n"
     ]
    }
   ],
   "source": [
    "# Initialize Counter object from list of words_ns\n",
    "counter = Counter(words_ns)\n",
    "\n",
    "# Find top 10 most common words\n",
    "top_10 = counter.most_common(10)\n",
    "\n",
    "# Print top 10\n",
    "print(top_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can say now that the most frequently used word in The Great Gatsby is **'Gatsby'** :D"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
